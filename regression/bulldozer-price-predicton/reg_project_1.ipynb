{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "69a7bd01",
   "metadata": {},
   "source": [
    "# Bulldozer sale price prediction \n",
    "In this notebook, we're going to go through an example machine learning project with the goal of predicting sale price of bulldozer.\n",
    "#### 1. Problem Defination\n",
    "> how well can we predict the future sale price of a bulldozer, given its characteristics and previous example of how much similar bulldozers have been sold so far.\n",
    "#### 2. Data\n",
    "A brief info about data -\n",
    "\n",
    "* Train.csv is the training set, which contains data through the end of 2011.\n",
    "* Valid.csv is the validation set, which contains data from January 1, 2012 - April 30, 2012 You make predictions on this set throughout the majority of the competition. Your score on this set is used to create the public leaderboard.\n",
    "* Test.csv is the test set, which won't be released until the last week of the competition. It contains data from May 1, 2012 - November 2012. Your score on the test set determines your final rank for the competition.\n",
    "\n",
    "#### 3. Evaluation\n",
    "* The evaluation metric for this competition is the RMSLE (root mean squared log error) between the actual and predicted auction prices.\n",
    "\n",
    "* NOTE - The goal for most regression evaluation metrics is to minimize the error. For example, my goal for this project will be to build a machine learning model which minimizes RMSLE.\n",
    "#### 4. Features "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bddc59e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7642508d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing training and validation sets\n",
    "df = pd.read_csv('data/TrainAndValid.csv', low_memory=False)\n",
    "len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b80caec",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "425e1dad",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32eb00db",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c611aaff",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "ax.scatter(df['saledate'][:1000], df['SalePrice'][:1000]);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85499f49",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['SalePrice'].hist()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "560c355c",
   "metadata": {},
   "source": [
    "### Parsing Dates\n",
    "When we work with time series data, we want to enrich the time and date component as much as possible. \n",
    "\n",
    "We can do that by tellin pandas which of our columns has dates in it using the `parse_dates` parameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76b88b99",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import data again but this time parse dates\n",
    "df = pd.read_csv('data/TrainAndValid.csv', parse_dates=['saledate'], low_memory=False)\n",
    "df.saledate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e969ba13",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.saledate.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db8d7293",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "ax.scatter(df['saledate'][:1000], df['SalePrice'][:1000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d1a3738",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head().T"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "752600af",
   "metadata": {},
   "source": [
    "### Sorting Dataframe by saledate \n",
    "When working with time series date, it's good idea to sort it by date."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8722afe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sort dataframe in date order\n",
    "df.sort_values('saledate', inplace=True)\n",
    "df.saledate.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0726104b",
   "metadata": {},
   "source": [
    "### Make a copy of the original dataFrame\n",
    "We make a copy of the original dataframe so that when we manipulate copy, we've still got our original data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cfc517f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_temp = df.copy() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6b6ecf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_temp.saledate"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebdb9e28",
   "metadata": {},
   "source": [
    "### Adding datetime parameter for `saledate` column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "616e25b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_temp['saleYear'] = df_temp.saledate.dt.year\n",
    "df_temp['saleMonth'] = df_temp.saledate.dt.month\n",
    "df_temp['saleDay'] = df_temp.saledate.dt.day\n",
    "df_temp['saleDayOfWeek'] = df_temp.saledate.dt.dayofweek\n",
    "df_temp['saleDayOfYear'] = df_temp.saledate.dt.dayofyear "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbe1486b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_temp.head().T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07661767",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we've enriched our dataframe with datetime features, we can remove saledate column\n",
    "df_temp.drop('saledate', axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ca35407",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking values of different columns\n",
    "df_temp.state.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "616b1f2a",
   "metadata": {},
   "source": [
    "### Converting Strings into categories\n",
    "One way we can turn all of our data into numbers is by converting them into  pandas categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d04b51b",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.api.types.is_string_dtype(df_temp['UsageBand'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "958f9f36",
   "metadata": {},
   "outputs": [],
   "source": [
    "for label, content in df_temp.items():\n",
    "    if pd.api.types.is_string_dtype(content): \n",
    "        print(label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dbf667c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Changing strings into categories\n",
    "for label, content in df_temp.items():\n",
    "    if pd.api.types.is_string_dtype(content):\n",
    "        df_temp[label] = content.astype('category').cat.as_ordered()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df9c6bd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_temp.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0369d63",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_temp['state'].dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "126cb80a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking missing data\n",
    "df_temp.isnull().sum() / len(df_temp)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb128169",
   "metadata": {},
   "source": [
    "### Fill missing values\n",
    "#### Filling numeric missing values first"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a226f16b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking which numerical colums have missing values\n",
    "for label, content in df_temp.items():\n",
    "    if pd.api.types.is_numeric_dtype(content):\n",
    "        if pd.isnull(content).sum():\n",
    "            print(label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "471b90d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filling numerical columns with missing values\n",
    "for label, content in df_temp.items():\n",
    "    if pd.api.types.is_numeric_dtype(content):\n",
    "        if pd.isnull(content).sum():\n",
    "            # Add a binary column which tells us if data was missing\n",
    "            df_temp[label + \"_is_missing\"] = pd.isnull(content)\n",
    "            # Filling missing values with median\n",
    "            df_temp[label] = content.fillna(content.median() )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee714e6c",
   "metadata": {},
   "source": [
    "### Filling and turnig categorical variables into numbers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64bc5ab0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check for columns with categorical values\n",
    "for label, content in df_temp.items():\n",
    "    if not pd.api.types.is_string_dtype(content):\n",
    "        print(label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7bb1860",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Turn categorical values into numbers and fill the missing\n",
    "for label, content in df_temp.items():\n",
    "    if not pd.api.types.is_numeric_dtype(content):\n",
    "        #  Add a binary column to indicate if the column previously had missin values \n",
    "        df_temp[label + \"_is_missing\"] = pd.isnull(content)\n",
    "        # turning categories into numbers \n",
    "        df_temp[label] = pd.Categorical(content).codes + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfc28e5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_temp.head().T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a022e7a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_temp.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc724019",
   "metadata": {},
   "source": [
    "### 5. Modelling\n",
    "Now that all of our data is numeric and has no missing values in it, we should be now able to build a machine learning model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "639c0928",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Let's make our machine learning model\n",
    "from sklearn.ensemble import RandomForestRegressor \n",
    "# Instantiating the model\n",
    "model = RandomForestRegressor()\n",
    "# Splitting the data into x and y\n",
    "x = df_temp.drop('SalePrice', axis=1)\n",
    "y = df_temp['SalePrice']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a848c2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splittin data into train and validation sets \n",
    "df_val = df_temp[df_temp.saleYear == 2012]\n",
    "df_train = df_temp[df_temp.saleYear != 2012]\n",
    "x_train, y_train = df_train.drop('SalePrice', axis=1), df_train.SalePrice\n",
    "x_valid, y_valid = df_val.drop('SalePrice', axis=1), df_val.SalePrice"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "342d1cfa",
   "metadata": {},
   "source": [
    "### Building a custom Evaluation function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72185a4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Evaluation Function (The competition uses RMSLE)\n",
    "from sklearn.metrics import mean_squared_log_error, mean_absolute_error, r2_score\n",
    "def rmsle(y_test, y_preds):\n",
    "    \"\"\"Calculate RootMeanSquaredLogError between predictions and true labels.\"\"\"\n",
    "    return np.sqrt(mean_squared_log_error(y_test, y_preds))\n",
    "\n",
    "# Create a function to evaluate model on different levels\n",
    "def show_scores(model):\n",
    "    train_preds = model.predict(x_train)\n",
    "    valid_preds = model.predict(x_valid) \n",
    "    scores = {\n",
    "        'Training MAE': mean_absolute_error(y_train, train_preds),\n",
    "        'Valid MAE': mean_absolute_error(y_valid, valid_preds),\n",
    "        'Training RMSLE': rmsle(y_train, train_preds),\n",
    "        'Valid RMSLE': rmsle(y_valid, valid_preds),\n",
    "        'Training R^2': r2_score(y_train, train_preds),\n",
    "        'Valid R^2': r2_score(y_valid, valid_preds)\n",
    "    }\n",
    "    \n",
    "    return scores"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff7af8ef",
   "metadata": {},
   "source": [
    "### Testing our model on a subset (To tune Hyperparameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31b2cddd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change max_samples value\n",
    "model = RandomForestRegressor(n_jobs=-1 , random_state=42, max_samples=10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf0f632d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fitting the model\n",
    "model.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf2e06d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluating the model using custom evaluation function\n",
    "show_scores(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "699ae8ac",
   "metadata": {},
   "source": [
    "### HyperParameter Tuning with RandomizedSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9729e3fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "# Evaluating model(RandomForestRegressor) by changing different HyperParameters through RandomSearchCV\n",
    "rs_grid = {\n",
    "    'n_estimators': np.arange(10, 100, 10),\n",
    "    'max_depth': [None, 3, 5, 10],\n",
    "    'min_samples_split': np.arange(2, 20, 2),\n",
    "    'min_samples_leaf': np.arange(1, 20, 2),\n",
    "    'max_features': [0.5, 1, 'sqrt', 'log2'],\n",
    "    'max_samples': [10000]\n",
    "    }\n",
    "rs_model = RandomizedSearchCV(\n",
    "    RandomForestRegressor(n_jobs=-1, random_state=42), \n",
    "    param_distributions=rs_grid,\n",
    "    n_iter=5,\n",
    "    cv=5,\n",
    "    verbose=True,\n",
    "    error_score='raise'\n",
    "    )\n",
    "# Fitting RandomizedSearchCV model\n",
    "rs_model.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7501a2f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Best hyperParameters \n",
    "rs_model.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9de35926",
   "metadata": {},
   "outputs": [],
   "source": [
    "show_scores(rs_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a089ed1",
   "metadata": {},
   "source": [
    "### Train a model with best hyperparameters\n",
    "NOTE : These were found after 100 iterations of `RandomizedSearchCV`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ff39fd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Most ideal hyperparameters\n",
    "ideal_model = RandomForestRegressor(n_estimators=40,\n",
    "                                    min_samples_leaf=1,\n",
    "                                    min_samples_split=14,\n",
    "                                    max_features=0.5,\n",
    "                                    n_jobs=-1,\n",
    "                                    max_samples=None,\n",
    "                                    random_state=42)\n",
    "# Fiting ideal model\n",
    "ideal_model.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b9aa4d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scores for ideal model, trained on all the data \n",
    "show_scores(ideal_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9841afbc",
   "metadata": {},
   "source": [
    "### Make predictions on test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9497ccb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing test dataset\n",
    "df_test = pd.read_csv('data/Test.csv', parse_dates=['saledate'], low_memory=False)\n",
    "df_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51a24a4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Make predictions on test_dataset\n",
    "# test_predicts = ideal_model.predict(df_test)  # Gives an error because the dataFrame on which model was trained and this dataFrame have different values and column count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a72fa853",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a42b00a",
   "metadata": {},
   "source": [
    "### Preprocessing our data (ie.. getting the test dataset into the same format as our training dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e43ae60",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_data(df):\n",
    "    \"\"\"Performs Transformations on a df and returns Transformed df\"\"\"\n",
    "    # add datetime parameter for saledate column\n",
    "    # df[\"saleYear\"] = df.saledate.dt.year\n",
    "    # df[\"saleMonth\"] = df.saledate.dt.month\n",
    "    # df[\"saleDay\"] = df.saledate.dt.day\n",
    "    # df[\"saleDayOfWeek\"] = df.saledate.dt.dayofweek\n",
    "    # df[\"saleDayOfYear\"] = df.saledate.dt.dayofyear\n",
    "    # df.drop(\"saledate\", axis=1, inplace=True)\n",
    "\n",
    "    # Convert string dtypes into categorical dytpe\n",
    "    for label, content in df.items():\n",
    "        if pd.api.types.is_string_dtype(content):\n",
    "            df[label] = content.astype(\"category\").cat.as_ordered()\n",
    "\n",
    "    #  Fill missing numerical rows with median\n",
    "    for label, content in df.items():\n",
    "        if pd.api.types.is_numeric_dtype(content):\n",
    "            if pd.isnull(content).sum():\n",
    "                # Add a binary column which tells us if data was missing\n",
    "                df[label + \"_is_missing\"] = pd.isnull(content)\n",
    "                # Filling missing values with median\n",
    "                df[label] = content.fillna(content.median())\n",
    "\n",
    "        # Fill categorical missing data and turn categories into numbers\n",
    "        if not pd.api.types.is_numeric_dtype(content):\n",
    "            df[label + '_is_missing'] = pd.isnull(content)\n",
    "            # We add + 1 to category codes because pandas encodes null value as -1 and we want 0\n",
    "            df[label] = pd.Categorical(content).codes + 1\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4ff9313",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test_copy = df_test.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbefad4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process the test data\n",
    "preprocess_data(df_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1687ca9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73291b49",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can find how columns differe by usings sets\n",
    "set(x_train.columns) - set(df_test.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "046198c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we have to manually adjust df_test to have auctioneerID_is_missing and set it to False.df_test\n",
    "df_test['auctioneerID_is_missing'] = False\n",
    "df_test = df_test.reindex(columns=list(x_train.columns))\n",
    "df_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72312d31",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Now we can make predictions as df_test is in the same format as ther training data\n",
    "test_preds = ideal_model.predict(df_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bca5c847",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_preds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4674cb8",
   "metadata": {},
   "source": [
    "We've made some preditions but they're in not the same format in which kaggle wants."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13a197c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Format the preidctions in the same format which kaggle is after\n",
    "df_preds = pd.DataFrame()\n",
    "df_preds['SalesID'] = df_test.SalesID\n",
    "df_preds['SalesPrice'] = df_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0504796",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dc44efd",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_preds.to_csv('data/test_predictions_practice.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "291f07f0",
   "metadata": {},
   "source": [
    "### Feature Importance\n",
    "Feature importance seeks to figure out which different attributes of data are most important when it comes to predicting the target variable (salePrice)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8287596",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find feature importance of our best model\n",
    "ideal_model.feature_importances_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b08a2c2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# helper function for plotting feature importance\n",
    "def plot_features(columns, importances, n=20):\n",
    "    df = (\n",
    "        pd.DataFrame({\"features\": columns, \"feature_importances\": importances})\n",
    "        .sort_values(\"feature_importances\", ascending=False)\n",
    "        .reset_index(drop=True)\n",
    "    )\n",
    "\n",
    "    # Plot dataframe\n",
    "    fig, ax = plt.subplots()\n",
    "    ax.barh(df[\"features\"][:n], df[\"feature_importances\"][:n])\n",
    "    ax.set_ylabel(\"Features\")\n",
    "    ax.set_xlabel(\"Feature Importance\")\n",
    "    ax.invert_yaxis()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5238c0eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_features(x_train.columns, ideal_model.feature_importances_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "322d33fb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
